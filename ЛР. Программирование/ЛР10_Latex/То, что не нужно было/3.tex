\documentclass[a4paper,12pt]{article}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{setspace}

% Форматирование основного текста
\setlength{\parindent}{1.25cm} % абзацный отступ
\setlength{\parskip}{6pt} % интервал между абзацами
\raggedright % выравнивание по левому краю

% Форматирование заголовков разделов
\usepackage{titlesec}
\titleformat{\section}{\raggedright\bfseries\fontsize{14pt}{14pt}\selectfont}{\thesection}{1em}{}
\titleformat{\subsection}{\raggedright\bfseries\fontsize{12pt}{12pt}\selectfont}{\thesubsection}{1em}{}
\titlespacing{\section}{0pt}{6pt}{6pt}
\titlespacing{\subsection}{0pt}{6pt}{6pt}

% Сноска
\usepackage{footnote}
\makesavenoteenv{tabular}

\begin{document}

% Титульная часть
\begin{center}
    {\LARGE\bfseries Методы оптимизации в машинном обучении}
    
    \vspace{1cm}
    
    {\large Иванов П.С., Петрова А.К.}
    
    \vspace{0.5cm}
    
    {\it Научный руководитель: к.т.н., доц. Сидоров В.П.}
\end{center}

\vspace{1cm}

\begin{abstract}
    В статье рассматриваются современные методы оптимизации, применяемые в задачах машинного обучения. Особое внимание уделяется алгоритмам градиентного спуска и их модификациям. Приводятся сравнительные характеристики различных подходов к оптимизации нейронных сетей.
\end{abstract}

\section*{Введение}

Машинное обучение\footnote{Machine learning — раздел искусственного интеллекта} стало неотъемлемой частью современных технологий. Одной из ключевых задач при построении моделей машинного обучения является оптимизация функции потерь~\cite{bishop2006}. 

Эффективные методы оптимизации позволяют значительно ускорить процесс обучения моделей и улучшить их качество. В последние годы появилось множество новых алгоритмов, адаптированных для работы с большими объемами данных~\cite{goodfellow2016}.

Актуальность темы обусловлена растущим интересом к глубокому обучению и необходимостью разработки эффективных методов тренировки сложных моделей.

\section{Основные методы оптимизации}

\subsection{Градиентный спуск}

Градиентный спуск является одним из наиболее распространенных методов оптимизации в машинном обучении. Основная идея заключается в итеративном обновлении параметров модели в направлении, противоположном градиенту функции потерь:

\[
\theta_{t+1} = \theta_t - \eta \nabla J(\theta_t)
\]

где \(\eta\) — скорость обучения, \(\nabla J(\theta_t)\) — градиент функции потерь.

\textbf{«Градиентный спуск остается фундаментальным методом оптимизации, несмотря на появление более сложных алгоритмов»} — отмечают исследователи в области машинного обучения.

\subsection{Стохастический градиентный спуск}

Стохастический градиентный спуск (SGD) является модификацией базового алгоритма, которая использует не весь набор данных для вычисления градиента, а лишь отдельные примеры или мини-батчи:

\[
\theta_{t+1} = \theta_t - \eta \nabla J(\theta_t; x_i, y_i)
\]

Это позволяет значительно ускорить процесс обучения на больших наборах данных~\cite{bottou2018}.

\subsection{Адаптивные методы}

К адаптивным методам оптимизации относятся алгоритмы, которые автоматически настраивают скорость обучения для каждого параметра модели. Наиболее известными представителями являются:

\begin{itemize}
    \item AdaGrad
    \item RMSProp
    \item Adam
\end{itemize}

Алгоритм Adam в настоящее время является одним из наиболее популярных методов оптимизации в глубоком обучении.

\section{Сравнительный анализ}

Проведенные эксперименты показывают, что различные методы оптимизации демонстрируют разную эффективность в зависимости от конкретной задачи и архитектуры модели.

Для простых выпуклых функций лучше всего подходит стандартный градиентный спуск, в то время как для невыпуклых задач, характерных для глубоких нейронных сетей, более эффективными оказываются адаптивные методы.

\section*{Заключение}

В статье рассмотрены основные методы оптимизации, применяемые в машинном обучении. Показано, что выбор алгоритма оптимизации существенно влияет на скорость сходимости и качество конечной модели.

Перспективным направлением исследований является разработка специализированных методов оптимизации для конкретных архитектур нейронных сетей и типов данных.

\begin{thebibliography}{9}
\bibitem{bishop2006} 
\textit{Bishop C. M.} 
Pattern Recognition and Machine Learning. — Springer, 2006.

\bibitem{goodfellow2016} 
\textit{Goodfellow I., Bengio Y., Courville A.} 
Deep Learning. — MIT Press, 2016.

\bibitem{bottou2018} 
\textit{Bottou L., Curtis F. E., Nocedal J.} 
Optimization Methods for Large-Scale Machine Learning // SIAM Review. — 2018. — Vol. 60. — P. 223–311.
\end{thebibliography}

\end{document}